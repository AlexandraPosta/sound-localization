{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate data\n",
    "\n",
    "This project uses the CMU_ARCTIC database, which is a speech database in CMU_ARCTIC speech synthesis databases, established by the Language Technologies Institute of Carnegie Mellon University, USA. This database is mainly used in the research of speech synthesis. The content of the corpus database was selected by the non-copyright center of Project Gutenberg, which is about 1150 sentences. An audio of two males and two females with American English accents were collected. The recording format is 16 bits, the sampling rate is 32 KHz, and the length of each sentence is 3 seconds. The database has a total of 4528 audio files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import pyroomacoustics as pra\n",
    "from pyroomacoustics.utilities import normalize\n",
    "from pyroomacoustics.transform import stft\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label resolution of angles\n",
    "RESOLUTION = 25\n",
    "\n",
    "# Number of samples to include while creating one ML feature\n",
    "SAMPLES = 2048\n",
    "\n",
    "# Determines the overlap of samples between consecutive features\n",
    "STEP = 1024\n",
    "\n",
    "# Training rooms dimensions\n",
    "ROOMS = {\n",
    "    'small' : np.array([4, 4, 3]),\n",
    "    'medium' : np.array([6, 6, 3]),\n",
    "    'large' : np.array([8, 8, 3])\n",
    "}\n",
    "\n",
    "# Testing rooms dimensions\n",
    "TEST_ROOMS = {\n",
    "    'small' : np.array([5, 5, 2]),\n",
    "    'medium' : np.array([7, 7, 2]),\n",
    "    'large' : np.array([9, 9, 2])\n",
    "}\n",
    "\n",
    "AUDIO_PATH = 'C:\\\\Users\\\\Alex\\\\source\\\\repos\\\\sound-localization\\\\data\\\\training'\n",
    "\n",
    "# Number of microphones\n",
    "MICS_NUMBER = 2\n",
    "MIC_COMBS = len(list(combinations(range(MICS_NUMBER), 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition for rooms, microphones and sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simulation_room(room_dim=[4, 4, 3], mic_pos=[2, 2, 1.5], room_fs=16000):    \n",
    "    # Initialize room\n",
    "    room = pra.ShoeBox(room_dim, fs=room_fs)\n",
    "    w = room_dim[0]\n",
    "    l = room_dim[1]\n",
    "    h = room_dim[2]\n",
    "\n",
    "    # Generate the microphones\n",
    "    mic_loc = np.c_[[w/2+0.2, l/2, h/2],[w/2-0.2, l/2, h/2]] \n",
    "    room.add_microphone_array(mic_loc)\n",
    "    \n",
    "    return room\n",
    "\n",
    "\n",
    "def create_sound_sources(room_dim=[4, 4, 3], resolution=RESOLUTION):        \n",
    "    # Specify angle in distance ranges\n",
    "    angle_range = range(0, 360, resolution)\n",
    "    dist_range = [1., 2.]\n",
    "    height_range = [room_dim[-1]/2]\n",
    "    sources = defaultdict(list)\n",
    "\n",
    "    for angle in angle_range:\n",
    "        for R in dist_range:\n",
    "            for h in height_range:\n",
    "                source = [R * math.cos(math.radians(angle)) + room_dim[0] / 2, \n",
    "                            R * math.sin(math.radians(angle)) + room_dim[0] / 2, h]\n",
    "                sources[angle].append((R, h, source))\n",
    "    return sources\n",
    "\n",
    "\n",
    "def simulate_room(audio_file, subset, room_type='small', mic_pos=[2, 2, 1.5], label_res=20):\n",
    "    file_name = os.path.basename(audio_file).split('.')[0]\n",
    "    \n",
    "    # Read the audio file\n",
    "    fs, audio = wavfile.read(audio_file)\n",
    "\n",
    "    # Get room dimensions\n",
    "    if subset == 'train':\n",
    "        room_dim = ROOMS[room_type]\n",
    "    else:\n",
    "        room_dim = TEST_ROOMS[room_type]\n",
    "\n",
    "    # Create all sound sources\n",
    "    sources = create_sound_sources(room_dim)\n",
    "\n",
    "    # Simulate every sound source that was created before\n",
    "    for angle, sources in sources.items():\n",
    "        if angle % label_res == 0:\n",
    "            print(f'\\nSimulating {angle}-{angle + label_res - 1} degrees:', end=' ')\n",
    "            \n",
    "        sys.stdout.write(\"#\")\n",
    "        \n",
    "        wav_signals = defaultdict(list)\n",
    "        \n",
    "        for dist, height, source in sources:\n",
    "            room = create_simulation_room(room_dim=room_dim, mic_pos=mic_pos, room_fs=fs)\n",
    "            room.add_source(source, signal=audio, delay=0.0)\n",
    "            room.simulate()\n",
    "\n",
    "            # Extract simulated signal for each microphone\n",
    "            data = room.mic_array.signals.T\n",
    "            data = np.array(normalize(data, bits=16), dtype=np.int16)\n",
    "            \n",
    "            # Append simulated data to the final WAV signals\n",
    "            wav_signals[dist].extend(data)\n",
    "            \n",
    "        # Save WAV files containing 2 channel data for all distances at given angle\n",
    "        for dist in wav_signals:\n",
    "            save_name = f'{subset}_angle_{angle}_dist_{int(dist*100)}_room_{room_type}_{file_name}.wav'\n",
    "            wavfile.write(f'{AUDIO_PATH}/{save_name}', fs, np.array(wav_signals[dist]))\n",
    "        \n",
    "        if angle % label_res == label_res - 1:\n",
    "            print(' Done.')\n",
    "            \n",
    "    print('\\nSimulation successful!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Simulating training data:')\n",
    "for room, dim in ROOMS.items():\n",
    "    # Call the function above and store all results\n",
    "    print(f'Room: {room}')\n",
    "    sound_files = \"C:\\\\Users\\\\Alex\\\\source\\\\repos\\\\Data\\\\Sound\\\\arctic_a0014.wav\"\n",
    "    simulate_room(sound_files, 'train', room, dim / 2, label_res=60)\n",
    "\n",
    "\n",
    "print('Simulating test data:')\n",
    "for room, dim in ROOMS.items():\n",
    "    # Call the function above and store all results\n",
    "    print(f'Room: {room}')\n",
    "    sound_files = \"C:\\\\Users\\\\Alex\\\\source\\\\repos\\\\Data\\\\Sound\\\\arctic_a0015.wav\"\n",
    "    simulate_room(sound_files, 'test', room, dim / 2, label_res=60)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, audio = wavfile.read(f'{AUDIO_PATH}/train_angle_0_dist_100_room_large_arctic_a0014.wav')\n",
    "cols = [f'mic_{i}' for i in range(2)]\n",
    "df = pd.DataFrame(data=audio, columns=cols, dtype=np.int16)\n",
    "df.iloc[2000:2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_dim = ROOMS['small']\n",
    "\n",
    "room = create_simulation_room(room_dim, mic_pos=room_dim / 2)\n",
    "sources = create_sound_sources(room_dim)\n",
    "\n",
    "def plot_room(room, sources):\n",
    "    fs, audio = wavfile.read(\"C:\\\\Users\\\\Alex\\\\source\\\\repos\\\\Data\\\\Sound\\\\arctic_a0010.wav\")\n",
    "\n",
    "    # Add every single source to the room\n",
    "    for angle, sources in sources.items():\n",
    "        for _, _, source in sources:\n",
    "            room.add_source(source, signal=audio, delay=0.5)\n",
    "\n",
    "    # Plot the room\n",
    "    fig, ax = room.plot()\n",
    "    fig.set_size_inches(6, 6)\n",
    "    ax.set_xlim([0, room_dim[0]])\n",
    "    ax.set_ylim([0, room_dim[1]])\n",
    "    ax.set_zlim([0, room_dim[2]])\n",
    "    \n",
    "plot_room(room, sources)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcc_phat(x_1, x_2, FS=16000, interp=1):    \n",
    "    n = len(x_1) + len(x_2) - 1\n",
    "    n += 1 if n % 2 else 0\n",
    "    \n",
    "    # Fourier transforms of the two signals\n",
    "    X_1 = np.fft.rfft(x_1, n=n)\n",
    "    X_2 = np.fft.rfft(x_2, n=n)\n",
    "    \n",
    "    # Normalize by the magnitude of FFT - because PHAT\n",
    "    np.divide(X_1, np.abs(X_1), X_1, where=np.abs(X_1) != 0)\n",
    "    np.divide(X_2, np.abs(X_2), X_2, where=np.abs(X_2) != 0)\n",
    "    \n",
    "    # GCC-PHAT = [X_1(f)X_2*(f)] / |X_1(f)X_2*(f)|\n",
    "    # See http://www.xavieranguera.com/phdthesis/node92.html for reference\n",
    "    CC = X_1 * np.conj(X_2)\n",
    "    cc = np.fft.irfft(CC, n=n * interp)\n",
    "        \n",
    "    # Maximum delay between a pair of microphones,\n",
    "    # expressed in a number of samples.\n",
    "    # 0.09 m is the mic array diameter and \n",
    "    # 340 m/s is assumed to be the speed of sound.\n",
    "    max_len = math.ceil(0.09 / 340 * FS * interp)\n",
    "    \n",
    "    # Trim the cc vector to only include a \n",
    "    # small number of samples around the origin\n",
    "    cc = np.concatenate((cc[-max_len:], cc[:max_len+1]))\n",
    "    \n",
    "    # Return the cross correlation\n",
    "    return cc\n",
    "\n",
    "\n",
    "def one_hot_encode(encoder, y_train, y_test):    \n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "    \n",
    "    # One-hot encode training and testing labels\n",
    "    enc = encoder.fit(y_train)\n",
    "    y_train = enc.transform(y_train)\n",
    "    y_test = enc.transform(y_test)\n",
    "    \n",
    "    return y_train, y_test\n",
    "\n",
    "\n",
    "def compute_gcc_matrix(observation, fs, interp=1):\n",
    "    \"\"\"\n",
    "    Creates a GCC matrix, where each row is a vector of GCC \n",
    "    between a given pair of microphones.\n",
    "    \"\"\" \n",
    "    \n",
    "    mic_pairs = combinations(range(MICS_NUMBER), r=2)\n",
    "\n",
    "    # Initialize a transformed observation, that will be populated with GCC vectors\n",
    "    # of the observation\n",
    "    transformed_observation = []\n",
    "\n",
    "    # Compute GCC for every pair of microphones\n",
    "    for mic_1, mic_2 in mic_pairs:\n",
    "        x_1 = observation[:, mic_1]\n",
    "        x_2 = observation[:, mic_2]\n",
    "\n",
    "        gcc = gcc_phat(x_1, x_2, FS=fs, interp=interp)\n",
    "\n",
    "        # Add the GCC vector to the GCC matrix\n",
    "        transformed_observation.append(gcc)    \n",
    "        \n",
    "    return transformed_observation\n",
    "\n",
    "\n",
    "\n",
    "def create_observations(wav_signals, room_size, fs, label, samples=1, step=1, resolution=RESOLUTION, music=False, interp=1):\n",
    "    # Lists of observations and labels that will be populated\n",
    "    X = []\n",
    "    y = []\n",
    "    rounded_label = round(label / resolution) * resolution\n",
    "    if rounded_label == 360: rounded_label = 0\n",
    "    \n",
    "    # Loop through the signal frame and take subframes\n",
    "    for i in range(0, len(wav_signals) - samples + 1, step):\n",
    "        y.append(rounded_label)\n",
    "        \n",
    "        # Extract the observation from subframe\n",
    "        observation = np.array(wav_signals[i : i + samples])\n",
    "        \n",
    "        if music:\n",
    "            # Transform observation into a STFT matrix\n",
    "            transformed_observation = compute_stft_matrix(observation)\n",
    "        else:\n",
    "            # Transform observation into a GCC matrix\n",
    "            transformed_observation = compute_gcc_matrix(observation, fs, interp=interp)\n",
    "            \n",
    "        X.append(transformed_observation)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def create_dataframe(subset, samples=20, step=5, resolution=RESOLUTION, interp=1):\n",
    "    dataframes = []\n",
    "    files = [f for f in os.listdir(AUDIO_PATH) if os.path.isfile(os.path.join(AUDIO_PATH, f))]\n",
    "\n",
    "    # Loop through all WAVs\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-3:] != 'wav':\n",
    "            continue\n",
    "    \n",
    "        print(f'{subset} file {i+1}/{len(files)}', end='\\r')\n",
    "        path = os.path.join(AUDIO_PATH, file)\n",
    "        fs, wav_signals = wavfile.read(path)\n",
    "\n",
    "        label = int(file.split('_')[2])\n",
    "\n",
    "        # Create observations from a given WAV file\n",
    "        X_temp, y_temp = create_observations(wav_signals, fs, label, samples, step, resolution, interp=interp)\n",
    "\n",
    "        cols = [\n",
    "                f'mics{mic_1+1}{mic_2+1}_{i}' \n",
    "                    for mic_1, mic_2 in combinations(range(MICS_NUMBER), r=2) \n",
    "                        for i in range(np.shape(X_temp)[2])\n",
    "            ]\n",
    "\n",
    "        df = pd.DataFrame(data=np.reshape(X_temp, (len(X_temp), -1)), columns=cols)\n",
    "        dist = int(file.split('_')[4])\n",
    "        room = file.split('_')[6]\n",
    "        df['dist'], df['room'] = dist, room\n",
    "            \n",
    "        # Add label column\n",
    "        df['label'] = y_temp\n",
    "        dataframes.append(df)\n",
    "        \n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "def create_whole_dataset(df_train, df_test, encoder, room=None, dist=None):\n",
    "    \"\"\"\n",
    "    Creates an entire dataset by extracting values from train and tests dataframes.\n",
    "    One-hot encodes the labels before returning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Can filter testing entries to only check performance\n",
    "    # for given conditions\n",
    "    if room:\n",
    "        df_test = df_test[df_test.room == room]\n",
    "    if dist:\n",
    "        df_test = df_test[df_test.dist == dist]\n",
    "    \n",
    "    # Create train/test observations\n",
    "    X_train = df_train.drop(columns=['dist', 'room', 'label']).values.reshape(\n",
    "        len(df_train), MIC_COMBS, -1)\n",
    "    X_test = df_test.drop(columns=['dist', 'room', 'label']).values.reshape(\n",
    "        len(df_test), MIC_COMBS, -1)\n",
    "    \n",
    "    # Create train/test labels\n",
    "    y_train, y_test = one_hot_encode(\n",
    "        encoder, df_train['label'].values, df_test['label'].values)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_dataframe('train', samples=SAMPLES, step=STEP, resolution=RESOLUTION)\n",
    "print()\n",
    "df_test = create_dataframe('test', samples=SAMPLES, step=STEP, resolution=RESOLUTION)\n",
    "print()\n",
    "\n",
    "df_train.to_csv('../training_data/azimuth_train_dataset.csv')\n",
    "df_test.to_csv('../training_data/azimuth_test_dataset.csv')\n",
    "print('Subsets csv generated!')\n",
    "\n",
    "# Create numpy arrays with observations and one-hot labels\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X_train, y_train, X_test, y_test = create_whole_dataset(df_train, df_test, encoder)\n",
    "\n",
    "print(np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))\n",
    "pd.set_option('display.max_columns', 15)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../training_data/azimuth_train_dataset.csv', index_col=[0])\n",
    "df_test = pd.read_csv('../training_data/azimuth_test_dataset.csv', index_col=[0])\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "encoder.fit([[label] for label in df_train['label']])\n",
    "X_train, y_train, X_test, y_test = create_whole_dataset(df_train, df_test, encoder)\n",
    "np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stft_matrix(observation, nfft=256):    \n",
    "    # Default value for overlap\n",
    "    step = nfft // 2\n",
    "    \n",
    "    # Calculate multidimensional STFT and return\n",
    "    transformed_observation = stft.analysis(observation, L=nfft, hop=step)\n",
    "    return np.transpose(transformed_observation, axes=[2, 1, 0])\n",
    "\n",
    "\n",
    "def get_music_prediction(X, room_size='small', resolution=RESOLUTION, fs=16000, nfft=256):    \n",
    "    # Run MUSIC algorithm for DOA\n",
    "    x = ROOMS[room_size][0]/2\n",
    "    y = ROOMS[room_size][1]/2\n",
    "    z = ROOMS[room_size][2]/2\n",
    "    mic_loc=np.c_[[x + 0.2, y, z],[ x - 0.2, y, z]]\n",
    "\n",
    "    doa = pra.doa.MUSIC(mic_loc, fs, nfft, n_grid=(360 // resolution), dim=3)\n",
    "    doa.locate_sources(X)\n",
    "    \n",
    "    return round((doa.azimuth_recon[0] * 180 / math.pi))\n",
    "  \n",
    "\n",
    "\n",
    "def get_all_predictions(is_info=False, samples=20, step=5, resolution=20):    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    info = []\n",
    "    \n",
    "    files = [file for file in os.listdir(AUDIO_PATH) if 'test' in file]\n",
    "    \n",
    "    # Loop through all WAVs\n",
    "    for i, file in enumerate(files):\n",
    "        if file[-3:] != 'wav': \n",
    "            continue\n",
    "\n",
    "        path = os.path.join(AUDIO_PATH, file)\n",
    "        fs, wav_signals = wavfile.read(path)\n",
    "        label = int(file.split('_')[2])\n",
    "        \n",
    "        # Create observations from a given WAV file\n",
    "        X, y = create_observations(wav_signals, fs, label, samples, step, resolution, True)\n",
    "\n",
    "        dist = int(file.split('_')[4])\n",
    "        room = file.split('_')[6]\n",
    "        y = [(dist, room, label) for label in y]\n",
    "           \n",
    "        # Store actual and predicted labels\n",
    "        y_true.extend(y)\n",
    "        preds = [get_music_prediction(x, room_size=room, resolution=resolution, fs=fs) for x in X]\n",
    "        y_pred.extend(preds)\n",
    "        \n",
    "        print(f'File {i+1}/{len(files)}', end='\\r')\n",
    "       \n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    #info = y_true[:, :-1]\n",
    "    #y_true = list(y_true[:, -1].astype(int))\n",
    "    return y_true, y_pred, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_true, y_pred, class_names, font_scale=0.8):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix between actual and predicted labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    register_matplotlib_converters()\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=font_scale)\n",
    "    rcParams['figure.figsize'] = 22, 10\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(24, 20)) \n",
    "    ax = sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt=\".3f\", \n",
    "        cmap=sns.diverging_palette(220, 20, n=7),\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    plt.ylabel('Actual', fontsize=14)\n",
    "    plt.xlabel('Predicted', fontsize=14)\n",
    "    ax.set_xticklabels(class_names, fontsize=12)\n",
    "    ax.set_yticklabels(class_names, rotation=0, fontsize=12)\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    total = sum(min(abs(y_t - y_p), (360 - abs(y_t - y_p))) ** 2 for y_t, y_p in zip(y_true, y_pred))\n",
    "    rms = ((total / len(y_true)) ** 0.5)\n",
    "    \n",
    "    if type(rms) == np.ndarray:\n",
    "        rms = rms[0]\n",
    "        \n",
    "    return round(rms, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MUSIC prediction on the data set\n",
    "y_true, y_pred, info = get_all_predictions(True, samples=SAMPLES, step=STEP, resolution=RESOLUTION)\n",
    "#print(np.shape(y_true), np.shape(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MUSIC prediction on the data set\n",
    "y_true, y_pred, info = get_all_predictions(True, samples=SAMPLES, step=STEP, resolution=RESOLUTION)\n",
    "print(np.shape(y_true), np.shape(y_pred))\n",
    "\n",
    "# Plot confusion matrix and report accuracy\n",
    "#plot_cm(y_true, y_pred, np.unique(y_true))\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {round(accuracy, 3)}')\n",
    "print(f'RMSE: {rmse(y_true, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
